{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08084463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e225b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our data file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c038bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of every character in the text\n",
    "char_list = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8175b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts individual characters to integers (encoder)\n",
    "in_encode = {char: i for i, char in enumerate(char_list)}\n",
    "# This converts individual integers to chatacters (decoder)\n",
    "out_decode = {i: char for i, char in enumerate(char_list)}\n",
    "\n",
    "# Define our encoder and decoder as function\n",
    "encode = lambda string: [in_encode[char] for char in string]\n",
    "decode = lambda encoding: ''.join([out_decode[integer] for integer in encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d8b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hello world!\"))\n",
    "print(decode(encode(\"Hello World!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e465f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our entire text as a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7353270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into train and test (video uses 0.9)\n",
    "train_data = data[:int(0.8*len(data))]\n",
    "val_data = data[int(0.8*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cc7ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a block size to train the model\n",
    "BLOCK_SIZE = 8\n",
    "# Set a batch size (number of random blocks we will select)\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = len(char_list)\n",
    "N_EMBED=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "365932cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fafee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc7d864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Make a token embedding table for each character\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, N_EMBED)\n",
    "        # Make a positional embedding table to keep track of relative char position\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBED)\n",
    "        # Add a linear layer from N_EMBED to VOCAB_SIZE\n",
    "        self.lm_head = nn.Linear(N_EMBED, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, ix, targets=None):\n",
    "        B, T = ix.shape\n",
    "        \n",
    "        # Gets a (B,T,C) embedding (Batch, Time, Channels)\n",
    "        # Use as logits (scores) for next character in sequence\n",
    "        token_embed = self.token_embedding_table(ix) # (B x T x C)\n",
    "        pos_embed = self.position_embedding_table(torch.arange(T)) # (T x C)\n",
    "        embed = token_embed + pos_embed\n",
    "        logits = self.lm_head(embed) # (B x T x VOCAB_SIZE)\n",
    "        \n",
    "        # We don't want any loss when generating\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # Collapse B and T into one dimension to fit the cross_entropy usage\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Check how well you predict the next char based on the logits\n",
    "            # The correct next token in the char's row should have a high value, and the rest should be low\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    # Generate the next tokens based on a given input\n",
    "    def generate(self, ix, max_new_tokens):\n",
    "        # ix = B x T array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions at the current index\n",
    "            logits, loss = self(ix)\n",
    "            # Look at just the last time step\n",
    "            logits = logits[:, -1, :] # Collapse logits to B x C\n",
    "            # Apply softmax to get probs\n",
    "            probs = F.softmax(logits, dim=-1) # B x C\n",
    "            # Sample \n",
    "            next_ix = torch.multinomial(probs, num_samples=1)\n",
    "            ix = torch.cat((ix, next_ix), dim=1)\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde32a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(len(char_list))\n",
    "logits, loss = model(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c604d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PyTorch optimizer\n",
    "# Smaller learning rates are usually used, but our model is small so we can get away with it\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dba24d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4161832332611084\n"
     ]
    }
   ],
   "source": [
    "for steps in range(40000):\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    \n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9516b581",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Generate 100 tokens, index the 0th row (get the single batch dimension) and convert to list for our decoder\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m generated \u001b[38;5;241m=\u001b[39m decode(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m, in \u001b[0;36mBigramLanguageModel.generate\u001b[0;34m(self, ix, max_new_tokens)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, ix, max_new_tokens):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# ix = B x T array of indices in current context\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# get the predictions at the current index\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m# Look at just the last time step\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# Collapse logits to B x C\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, ix, targets)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Gets a (B,T,C) embedding (Batch, Time, Channels)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Use as logits (scores) for next character in sequence\u001b[39;00m\n\u001b[1;32m     20\u001b[0m token_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding_table(ix) \u001b[38;5;66;03m# (B x T x C)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m pos_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (T x C)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m embed \u001b[38;5;241m=\u001b[39m token_embed \u001b[38;5;241m+\u001b[39m pos_embed\n\u001b[1;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(embed) \u001b[38;5;66;03m# (B x T x VOCAB_SIZE)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Start off with a newline character\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "# Generate 100 tokens, index the 0th row (get the single batch dimension) and convert to list for our decoder\n",
    "generated = decode(model.generate(idx, max_new_tokens=500)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd4723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
