{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "08084463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e225b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our data file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c038bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of every character in the text\n",
    "char_list = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8175b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This converts individual characters to integers (encoder)\n",
    "in_encode = {char: i for i, char in enumerate(char_list)}\n",
    "# This converts individual integers to chatacters (decoder)\n",
    "out_decode = {i: char for i, char in enumerate(char_list)}\n",
    "\n",
    "# Define our encoder and decoder as function\n",
    "encode = lambda string: [in_encode[char] for char in string]\n",
    "decode = lambda encoding: ''.join([out_decode[integer] for integer in encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "19d8b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hello world!\"))\n",
    "print(decode(encode(\"Hello World!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69e465f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our entire text as a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b7353270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into train and test (video uses 0.9)\n",
    "train_data = data[:int(0.8*len(data))]\n",
    "val_data = data[int(0.8*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1cc7ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a block size to train the model\n",
    "BLOCK_SIZE = 8\n",
    "# Set a batch size (number of random blocks we will select)\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = len(char_list)\n",
    "N_EMBED=32\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "365932cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7fafee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fc7d864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Make a token embedding table for each character\n",
    "        self.token_embedding_table = nn.Embedding(VOCAB_SIZE, N_EMBED)\n",
    "        # Make a positional embedding table to keep track of relative char position\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBED)\n",
    "        # Add a linear layer from N_EMBED to VOCAB_SIZE\n",
    "        self.lm_head = nn.Linear(N_EMBED, VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, ix, targets=None):\n",
    "        B, T = ix.shape\n",
    "        \n",
    "        # Gets a (B,T,C) embedding (Batch, Time, Channels)\n",
    "        # Use as logits (scores) for next character in sequence\n",
    "        token_embed = self.token_embedding_table(ix) # (B x T x C)\n",
    "        pos_embed = self.position_embedding_table(torch.arange(T, device=DEVICE)) # (T x C)\n",
    "        embed = token_embed + pos_embed\n",
    "        logits = self.lm_head(embed) # (B x T x VOCAB_SIZE)\n",
    "        \n",
    "        # We don't want any loss when generating\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        \n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # Collapse B and T into one dimension to fit the cross_entropy usage\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Check how well you predict the next char based on the logits\n",
    "            # The correct next token in the char's row should have a high value, and the rest should be low\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    # Generate the next tokens based on a given input\n",
    "    def generate(self, ix, max_new_tokens):\n",
    "        # ix = B x T array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            ix_cond = ix[:, -BLOCK_SIZE:]\n",
    "            # get the predictions at the current index\n",
    "            logits, loss = self(ix_cond)\n",
    "            # Look at just the last time step\n",
    "            logits = logits[:, -1, :] # Collapse logits to B x C\n",
    "            # Apply softmax to get probs\n",
    "            probs = F.softmax(logits, dim=-1) # B x C\n",
    "            # Sample \n",
    "            next_ix = torch.multinomial(probs, num_samples=1)\n",
    "            ix = torch.cat((ix, next_ix), dim=1)\n",
    "        return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bde32a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(len(char_list))\n",
    "model = model.to(DEVICE)\n",
    "logits, loss = model(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c604d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PyTorch optimizer\n",
    "# Smaller learning rates are usually used, but our model is small so we can get away with it\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dba24d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8752593994140625\n"
     ]
    }
   ],
   "source": [
    "for steps in range(40):\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    \n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9516b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off with a newline character\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "# Generate 100 tokens, index the 0th row (get the single batch dimension) and convert to list for our decoder\n",
    "generated = decode(model.generate(idx, max_new_tokens=500)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d8ba0cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "T\n",
      "TAOv\n",
      "in s, st p cFhe Henonsnds\n",
      "Sa bofnertautoronaniz s he\n",
      "YtonLTerr ysL,\n",
      "A:\n",
      "Iod,ldiner e o:r l\n",
      "An wee. m o, beavthavMu bype mZfxxl\n",
      "Lauimre akveythen y lleaytoy, iste\n",
      "z?\n",
      "anx nsruneef hemy :\n",
      "Yl te yngstn ha f on\n",
      "Snihere tm the tuwnerend e\n",
      "N&av c, in f y eslHA\n",
      "LouOyofMatd lar beRBingo ser estdnen Rr hen b\n",
      "BhM, plbrere pe\n",
      "A ldarnnleslmyn - ory f\n",
      "\n",
      "Aneereshe\n",
      "LIase beral, sg y d erdo avt mesni!yne m st ayar, tnouESeyyofOGye wYoc3y! IIwre greeren\n",
      "Lt fUthoSayoUf the mi, a\n",
      "WA, ty  omuo beding tl yn s ra\n"
     ]
    }
   ],
   "source": [
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2814ce29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
